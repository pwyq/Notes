\documentclass[lang=en,mode=geye,device=normal,color=blue,14pt]{elegantnote}
\usepackage{amsmath,amssymb}
\usepackage{bbm}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{subfiles}
\usepackage{pythonhighlight}

\usepackage{xcolor}
\definecolor{shadecolor}{RGB}{150,150,150}
\newcommand{\mybox}[1]{\par\noindent\colorbox{shadecolor}
{\parbox{\dimexpr\textwidth-2\fboxsep\relax}{#1}}}

\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\1}{\mathbbm{1}}
\DeclareMathOperator*{\R}{\mathbbm{R}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Note: Reinforcement Learning - An Introduction}

\author{Yanqing Wu}
%\institute{Viwistar Robotics}

% \version{0.1.0}
\date{\today}

\begin{document}
\maketitle

\setlength{\parindent}{0pt}
\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%

\subfile{chapter_1.tex}

%%%%%%%%%%%%%%%%%%%%%%%

\subfile{chapter_2.tex}

%%%%%%%%%%%%%%%%%%%%%%%

\subfile{chapter_3.tex}

%%%%%%%%%%%%%%%%%%%%%%%

\subfile{chapter_4.tex}

%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section{Monte Carlo Methods}

MC, is for estimating value functions and discovering optimal policies. Unlike methods in previous chapters, MC does not use complete knowledge of the environment. MC only requires \textit{experience} - sample sequences of states, actions and rewards from actual or simulated interaction with an environment. That is, MC is a \textbf{model-free} learning method.

MC is based on averaging complete sample returns. Since we are considering sampling returns, we define MC only for episodic tasks (in this book) \footnote{We can never sample an actual return value in continuous tasks. We may get around by using artificial time horizon, or TD (with eligibility traces); however, all these incur bias. see \url{https://datascience.stackexchange.com/a/77789/137431}}. Once an episode ends, the value estimates and policies change. MC is thus a episode-by-episode update, not a step-by-step (online) update.

The procedure of MC is similar to DP: prediction problem (finding $v_\pi$ and $q_\pi$) -> policy improvement -> control problem (solve by GPI). Except that, in DP, we compute value functions based on model, here we learn value functions from sample returns.

\subsection{Monte Carlo Prediction}

To learn $v_\pi$, we average the returns observed after visits to that state. The more returns are observed, the more closer to the expected value (recall the Law of Large Numbers). Each occurrence of $s$ in an episode is called a \textit{visit} to $s$. First-visit to $s$ is the first time $s$ is visited in an episode.

\begin{tcolorbox}[width=1.1\textwidth,title={First-visit MC prediction, for estimating $V \approx v_\pi$}]
Input: policy $\pi$ (to be evaluated)
Initialize:

$\quad V(s) \in \R$, arbitrarily.
$Returns(s) \leftarrow$ an empty list, $\forall s \in \mathcal{S}$

Loop forever (for each episode):

$\quad$Generate an episode following $\pi : S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T$

$\quad G \leftarrow 0$

$\quad$Loop for each step of episode, $t = T-1, T-2, ..., 0:$

$\quad\quad G \leftarrow \gamma G + R_{t+1}$

$\quad\quad$if $S_t$ not appears in $S_0, S_1, ..., S_{t-1}:$

$\quad\quad\quad$Append $G$ to $Returns(S_t)$

$\quad\quad\quad V(S_t) \leftarrow$ average($Returns(s)$)
\end{tcolorbox}

\begin{tcolorbox}[width=1.1\textwidth,title={Every-visit MC prediction, for estimating $V \approx v_\pi$}]
Input: policy $\pi$ (to be evaluated)
Initialize:

$\quad V(s) \in \R$, arbitrarily.
$Returns(s) \leftarrow$ an empty list, $\forall s \in \mathcal{S}$

Loop forever (for each episode):

$\quad$Generate an episode following $\pi : S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T$

$\quad G \leftarrow 0$

$\quad$Loop for each step of episode, $t = T-1, T-2, ..., 0:$

$\quad\quad G \leftarrow \gamma G + R_{t+1}$

$\quad\quad$Append $G$ to $Returns(S_t)$

$\quad\quad V(S_t) \leftarrow$ average($Returns(s)$)
\end{tcolorbox}

The first-visit MC method estimates $v_\pi (s)$ as the average of the returns following first visit to $s$.

The every-visit MC method averages the returns following all visits to $s$. This method extends more naturally to function approximation (chap 9) and eligibility traces (chap 12).

Advantage of MC over DP: (1) no need the model (i.e. the transition dynamics); (2) can work with sample episodes \textit{alone}; (3) can estimate the target state only while ignoring all other states (MC does not bootstrap, meaning each state estimation is independent of other states).

\subsection{Monte Carlo Estimation of Action Values}

When without a model, $v(s)$ along cannot determine $\pi$; in this case, $q(s,a)$ is more useful. Thus, one of MC goals is to estimate $q_*$.

The only problem is that many $(s,a)$ pairs may never be visited. For example, if we have a deterministic policy we only get one action per state (the one that the policy favor). Hence, we only observe returns for one action. This is a problem of \textbf{maintaining exploration}.
One way to solve this is exploring starts.
\textit{exploring starts}: assume episodes start in a $(s,a)$ pair, and that every pair has a nonzero probability to be selected at the start. (This is sometimes useful). A more common way to go about it, is to only consider stochastic policies where the probability of every action in every state is not 0.

\begin{tcolorbox}[width=1.1\textwidth,title={MC Exploring Starts, for estimating $V \approx v_\pi$}]
Initialize:

$\quad \pi(s) \in \mathcal{A}(s)$ (arbitrarily), $\forall s in \mathcal{S}$

$\quad Q(s,a) \in \R$ (arbitrarily), $\forall s \in \mathcal{S}, a \in \mathcal{A}(s)$

$\quad Returns(s,a) \leftarrow$ empty list, $\forall s \in \mathcal{S}, a \in \mathcal{A}(s)$

Loop forever (for each episode):

$\quad$ Choose a random $(S_0, A_0)$ pair s.t. all pairs have nonzero probability

$\quad$ Generate an episode(SAR trajectory) from $(S_0, A_0)$, following $\pi$

$\quad G \leftarrow 0$

$\quad$ Loop for each step of episode, $t = T-1, T-2, ..., 0$:

$\quad\quad G\leftarrow \gamma G + R_{t+1}$

$\quad\quad$ if $(S_t, A_t)$ pair not in the episode:

$\quad\quad\quad$ Append $G$ to $Returns(S_t, A_t)$

$\quad\quad\quad Q(S_t, A_t) \leftarrow$ average$(Returns(S_t, A_t))$

$\quad\quad\quad \pi(S_t) \leftarrow \argmax_a Q(S_t, a)$

\end{tcolorbox}

\subsection{Monte Carlo Control}


We now look at how our MC estimation can be used in control. Meaning, to approximate optimal policies.

The idea is to follow generalized policy iteration (GPI), where we will maintain an approximate policy and an approximate value function.
We continuously alter the value function to be a better approximation for the policy, and the policy is continuously improved (see previous chapter).


The policy evaluation part is done exactly as described in the previous chapter, except that we are evaluating the state-action pair, rather than states.

The policy improvement part is done by taking greedy actions in each state. That is, for any action-value function $q$, and for every state $s$, the greedy policy chooses the action with maximal action-value:

$$ \pi(s) \doteq \argmax_a q(s,a) $$

\subsection{Monte Carlo Control without Exploring Starts}

To make sure that all actions are being selected infinitely often, we must continuously select them.
There are 2 approaches to ensure this â€” on-policy methods and off-policy methods.

\begin{definition}
\textbf{On-policy methods}: evaluate or improve the policy that is used to make decisions.
\end{definition}
\begin{definition}
\textbf{Off-policy method}: evaluate or improve a policy different from the one used to generate the data (make decisions).
\end{definition}


For example, MC Exploring Starts is a on-policy method.

\begin{definition}
\textbf{soft policy}: $\pi(a|s) > 0\quad \forall s \in \mathcal{S}$ and $\forall a \in \mathcal{A}(s).$
\end{definition}

\begin{definition}
\textbf{$\epsilon$-soft policy}: $\pi(a|s) \geq \frac{\epsilon}{|\mathcal{A}(s)|} \quad \forall s \in \mathcal{S}$ and $\forall a \in \mathcal{A}(s)$ and for some $\epsilon > 0$.
\end{definition}

For example, $\epsilon$-greedy policies are examples of $\epsilon$-soft policies. 
$\epsilon$-soft policy gradually moves closer to a deterministic optimal policy.

\begin{tcolorbox}[width=1.1\textwidth,title={On-policy first-visit MC control, estimates $V \approx v_\pi$}]
parameter: small $\epsilon$ > 0

Initialize:

$\quad \pi(s) \leftarrow$ an arbitrary $\epsilon$-soft policy

$\quad Q(s,a) \in \R$ (arbitrarily), $\forall s \in \mathcal{S}, a \in \mathcal{A}(s)$

$\quad Returns(s,a) \leftarrow$ empty list, $\forall s \in \mathcal{S}, a \in \mathcal{A}(s)$

Loop forever (for each episode):

$\quad$Generate an episode(SAR trajectory) following $\pi$

$\quad G \leftarrow 0$

$\quad$Loop for each step of episode, $t = T-1, T-2, ..., 0$:

$\quad\quad G\leftarrow \gamma G + R_{t+1}$

$\quad\quad$if $(S_t, A_t)$ pair not in the episode:

$\quad\quad\quad$Append $G$ to $Returns(S_t, A_t)$

$\quad\quad\quad Q(S_t, A_t) \leftarrow$ average$(Returns(S_t, A_t))$

$\quad\quad\quad A* \leftarrow \argmax_a Q(S_t, a) \quad$ (with ties broken arbitrarily)

$\quad\quad\quad$For all $a \in \mathcal{A}(S_t):$
$\quad\quad\quad\quad$ \begin{equation}
  \pi(a|S_t) \leftarrow
    \begin{cases}
      1 - \epsilon + \epsilon / |\mathcal{A}(S_t)| & \text{if $a = A*$ (greedy)}\\
      \epsilon / |\mathcal{A}(S_t)| & \text{if $a \neq A*$ (non-greedy)}
    \end{cases}       
\end{equation}

\end{tcolorbox}

\subsection{Off-policy Prediction via Importance Sampling}


\subsection{Learning Objectives (UA RL MOOC)}

Lesson 1: Introduction to Monte-Carlo Methods 

Understand how Monte-Carlo methods can be used to estimate value functions from sampled interaction 

Identify problems that can be solved using Monte-Carlo methods 

Use Monte-Carlo prediction to estimate the value function for a given policy. 

Lesson 2: Monte-Carlo for Control 

Estimate action-value functions using Monte-Carlo 

Understand the importance of maintaining exploration in Monte-Carlo algorithms 

Understand how to use Monte-Carlo methods to implement a GPI algorithm

Apply Monte-Carlo with exploring starts to solve an MDP 

Lesson 3: Exploration Methods for Monte-Carlo 

Understand why exploring starts can be problematic in real problems 

Describe an alternative exploration method for Monte-Carlo control 

Lesson 4: Off-policy learning for prediction 

Understand how off-policy learning can help deal with the exploration problem 

Produce examples of target policies and examples of behavior policies

Understand importance sampling 

Use importance sampling to estimate the expected value of a target distribution using samples from a different distribution

Understand how to use importance sampling to correct returns 

Understand how to modify the Monte-Carlo prediction algorithm for off-policy learning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subfile{chapter_extra_notes.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{references.bib}

\end{document}