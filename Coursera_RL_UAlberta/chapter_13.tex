\documentclass[sutton_barto_notes.tex]{subfiles}
\begin{document}

\newpage
\section{Policy Gradient Methods}

\textbf{Action-valued method}: learns the values of actions, then select actions based on estimated action values. If no action-value estimate, then no policy.

On the contrary, a \textbf{parameterized policy} can select actions w/o consulting a value function (a value function may still be used to learn the policy parameter, but isn't required for action selection).

\textbf{Policy gradient method}:
Let $J(\bm{\theta})$ be the scalar performance measure w.r.t policy parameter. We want to maximize performance (so gradient ascent, note the \textbf{plus} sign):
$$ \bm{\theta_{t+1}} = \bm{\theta_t} + \alpha \widehat{\nabla J(\bm{\theta_t})}$$
where $\widehat{\nabla J(\bm{\theta_t})}$ is a stochastic estimate whose expectation approximates the gradient of the performance measure w.r.t $\theta_t$. PG methods not necessarily learn an approximate value function.

\textbf{actor-critic method}: learn approximations of both policy (actor) and value functions (critic).

\begin{itemize}
\item episodic case: performance = the value of the start state under the parameterized policy $J(\bm{\theta}) \doteq v_{\pi_{\bm{\theta}}}(S_0)$
\item continuing case: performance = average reward rate
\end{itemize}

\subsection{Policy Approximation and its Advantages}

In PG methods, policy can be parameterized in any way, as long as the policy $\pi(a|s,\theta)$ is differentiable w.r.t parameters.

\begin{definition}
\textbf{soft-max in action preferences}: action preference $h(s,a,\bm{\theta})$ can be parameterized arbitrarily.
$$ \pi(a|s,\bm{\theta}) \doteq \frac{e^{h(s,a,\bm{\theta})}}{\sum_b e^h(s,b,\bm{\theta})}$$
\end{definition}
For example, the preference can be linear $h(s,a,\bm{\theta}) = \bm{\theta}^T \bm{x}(s,a)$

Advantages of PG:
\begin{itemize}
\item Action-value methods have no natural way of finding stochastic optimal policies, whereas PG methods can
\item policy may be simpler to approximate
\item policy parameterization is sometimes a good way to impart prior knowledge
\item action probabilities change smoothly
\item stronger convergence guarantees for PG than action-value methods
\begin{itemize}
	\item because taking the gradient update ensures that the updates are smooth whereas in the action-value selection the max can change the selected action suddenly.
\end{itemize}
\end{itemize}


\subsection{The Policy Gradient Theorem}

\begin{definition}
\textbf{policy gradient theorem}
$$\nabla J(\bm{\theta}) \propto \sum_s\mu(s)\sum_a q_\pi(s,a)\nabla\pi(a|s,\bm{\theta})$$
\end{definition}
In the episodic case, the constant of proportionality is the average length of an episode, and in the continuing case it is 1 (which is equality actually). $\mu$ is the distribution under $\pi$.

\subsection{REINFORCE: Monte Carlo Policy Gradient}

The RHS of the PG theorem is a sum over states weighted by how often the states occur under the target policy $\pi$.
\begin{align*}
\nabla J(\bm{\theta}) & \propto \sum_s\mu(s)\sum_a q_\pi(s,a)\nabla\pi(a|s,\bm{\theta}) \\
&= \E_\pi [\sum_a q_\pi(S_t, a)\nabla\pi(a|S_t, \bm{\theta})] \\
&= \E_\pi [\sum_a \pi(a|S_t,\bm{\theta}) q_\pi(S_t, a) \frac{\nabla\pi(a|S_t, \bm{\theta})}{\pi(a|S_t, \bm{\theta})} ] \\
&= \E_\pi [q_\pi(S_t, A_t) \frac{\nabla\pi(a|S_t, \bm{\theta})}{\pi(a|S_t, \bm{\theta})}] \\
&= \E_\pi [G_t \frac{\nabla\pi(a|S_t, \bm{\theta})}{\pi(a|S_t, \bm{\theta})}]
\end{align*}
Therefore, we have the REINFORCE update:
\begin{align*}
\bm{\theta_{t+1}} &\doteq \bm{\theta_t} + \alpha G_t \frac{\nabla\pi(A_t|S_t,\bm{\theta_t})}{\pi(A_t|S_t,\bm{\theta_t})} \\
&= \bm{\theta} + \alpha G_t \nabla ln \pi(A_t|S_t, \bm{\theta})
\end{align*}


The vector is the direction in parameter space that most increases the probability of repeating the action $A_t$ on future visits to state $S_t$. The update increases the parameter vector in this direction proportional to the return (because it causes the parameter to move most in the directions that favor actions that yield the highest return), and inversely proportional to the action probability (because we want to prevent same actions to not be constantly selected, resulting in local optimum).

\begin{tcolorbox}[width=1.1\textwidth,title={REINFORCE: Monte-Carlo Policy-Gradient Control (episodic) for $\pi_*$}]
Input: a differentiable policy parameterization $\pi(a|s,\bm{\theta})$

Algorithm parameter: step size $\alpha > 0$

Initialize policy parameter $\bm{\theta} \in \R^{d'}$

Loop forever (for each episode):

$\quad$Generate an episode $S_0,A_0,R_1,...,S_{T-1},A_{T-1},R_T$, following $\pi(\cdot|\cdot,\bm{\theta})$

$\quad$Loop for each step of the episode $t=0,1,...,T-1$:

$\quad\quad G\leftarrow\sum_{k=t+1}^T \gamma^{k-t-1}R_k$

$\quad\quad\bm{\theta}\leftarrow\bm{\theta}+\alpha\gamma^t G \nabla ln\pi(A_t | S_t, \bm{\theta})$
\end{tcolorbox}

\begin{itemize}
\item good convergence properties (update in the same direction as the performance gradient)
\item high variance and slow learning
\end{itemize}

\subsection{REINFORCE with Baseline}

We include an arbitrary baseline $b(s)$ (as long as it doesn't vary with $a$) to (1) strictly generalize REINFORCE; (2) reduce variances w/o introducing bias; (3) dynamically adjust to different states for action selection, (and thus faster learning).
$$\nabla J(\bm{\theta}) \propto \sum_s\mu(s)\sum_a (q_\pi(s,a) - b(s))\nabla\pi(a|s,\bm{\theta})$$
Therefore, the update rule is
\begin{align*}
\bm{\theta_{t+1}} &\doteq \bm{\theta_t} + \alpha (G_t - b(S_t)) \frac{\nabla\pi(A_t|S_t,\bm{\theta_t})}{\pi(A_t|S_t,\bm{\theta_t})} \\
&= \bm{\theta} + \alpha (G_t - b(S_t)) \nabla ln \pi(A_t|S_t, \bm{\theta})
\end{align*}
A natural choice for the baseline is an estimate of the state value, $\hat{v}(S_t, \bm{w})$.

\begin{tcolorbox}[width=1.1\textwidth,title={REINFORCE with baseline (episodic) for estimating $\pi_*$}]
Input: a differentiable policy parameterization $\pi(a|s,\bm{\theta})$

Input: a differentiable state-value function parameterization $\hat{v}(s,\bm{w})$

Algorithm parameter: step size $\alpha^{\bm{\theta}} > 0, \alpha^{\bm{w}} > 0$

Initialize policy parameter $\bm{\theta} \in \R^{d'}$ and state-value weight $\bm{w} \in \R^d$

Loop forever (for each episode):

$\quad$Generate an episode $S_0,A_0,R_1,...,S_{T-1},A_{T-1},R_T$, following $\pi(\cdot|\cdot,\bm{\theta})$

$\quad$Loop for each step of the episode $t=0,1,...,T-1$:

$\quad\quad G\leftarrow\sum_{k=t+1}^T \gamma^{k-t-1}R_k$

$\quad\quad \delta\leftarrow G - \hat{v}(S_t, \bm{w})$

$\quad\quad \bm{w} \leftarrow \bm{w} + \alpha^{\bm{w}}\delta\nabla\hat{v}(S_t,\bm{w})$

$\quad\quad\bm{\theta}\leftarrow\bm{\theta}+\alpha^{\bm{\theta}}\gamma^t \delta \nabla ln\pi(A_t | S_t, \bm{\theta})$
\end{tcolorbox}

REINFORCE with baseline is unbiased and converge asymptotically to a local minimum but it has a high variance (MC) and thus learns slowly.
And, itâ€™s inconvenient to use online or in continuing setting).


\subsection{Actor-Critic Methods}

\paragraph{why go from baseline methods to AC?} In REINFORCE with baseline, the learned $\hat{v}(s)$ estimates only the first state of each state transition. It is made prior to the transition's action and thus cannot be used to assess that action. In AC methods, the $\hat{v}(s)$ is also applied to the second state of the transition (which forms a one-step return; bootstrapping!), so we can assessing that action. This way of assessing actions is called a \textit{critic}.

Note that one-step return introduces bias, but it is often superior to the actual return for lower variance and easier computation. We can fix the bias with n-step returns and eligibility traces.

\begin{definition}
\textbf{one-step AC} replace the full return of REINFORCE with the one-step return. The baseline of REINFORCE may be a $\hat{v}(s)$, but AC must use $\hat{v}(s)$ as the baseline.
\begin{align*}
\bm{\theta_{t+1}} &\doteq \bm{\theta_t} + \alpha (\underbrace{G_{t:t+1}}_{\text{one-step return}} - \hat{v}(S_t, \bm{w})) \frac{\nabla\pi(A_t|S_t,\bm{\theta_t})}{\pi(A_t|S_t,\bm{\theta_t})} \\
&= \bm{\theta_t} + \alpha (R_{t+1} + \gamma\hat{v}(S_{t+1},\bm{w}) - \hat{v}(S_t, \bm{w})) \frac{\nabla\pi(A_t|S_t,\bm{\theta_t})}{\pi(A_t|S_t,\bm{\theta_t})} \\
\end{align*}
\end{definition}

Since we are using one-step return, the one-step AC is full online and incremental.

\begin{tcolorbox}[width=1.1\textwidth,title={One-step Actor-Critic (episodic), for estimating $\pi_\theta\approx\pi_*$}]
Input: a differentiable policy parameterization $\pi(a|s,\bm{\theta})$

Input: a differentiable state-value function parameterization $\hat{v}(s,\bm{w})$

Algorithm parameter: step size $\alpha^{\bm{\theta}} > 0, \alpha^{\bm{w}} > 0$

Initialize policy parameter $\bm{\theta} \in \R^{d'}$ and state-value weight $\bm{w} \in \R^d$

Loop forever (for each episode):

$\quad$Initialize $S$ (first state of episode)

$\quad I \leftarrow 1$

$\quad$Loop while $S$ != $terminal$ (for each time step):

$\quad\quad A \sim \pi(\cdot | S, \theta)$

$\quad\quad$Take action $A$, observe $S', R$

$\quad\quad \delta\leftarrow R + \hat{v}(S_{t+1},\bm{w}) - \hat{v}(S_t, \bm{w})$

$\quad\quad \bm{w} \leftarrow \bm{w} + \alpha^{\bm{w}}\delta\nabla\hat{v}(S_t,\bm{w})$

$\quad\quad\bm{\theta}\leftarrow\bm{\theta}+\alpha^{\bm{\theta}} I \delta \nabla ln\pi(A_t | S_t, \bm{\theta})$

$\quad\quad I \leftarrow \gamma I$

$\quad\quad S \leftarrow S'$
\end{tcolorbox}

AC with Eligibility Traces (episodic) is available in the book. (1) We just declare eligibility trace vectors for both $\bm{\theta}$ and $\bm{w}$, and a trace-decay rates $\lambda$. (2) We replace one-step return $G_{t:t+1}$ with n-step return $G_{t:t+n}$ or $\lambda$-return $G_t^\lambda$. (3) We decay the $\gamma$ for both ET vectors at each update.

\subsection{Policy Gradient for Continuing Problems}

Recall in chap 10.3, we introduce the average reward for continuing problems and a steady-state distribution $\mu(s)$.
$$\sum_s \mu(s) \sum_a \pi(a|s,\bm{\theta}) p(s'|s,a) = \mu(s')\quad \forall s' \in \S$$

The pseudocode of AC with ET (continuing) is similar to AC with ET (episodic), except that we (1) replace reward with (reward - mean reward), and the mean reward is update as $\overline{R}\leftarrow\overline{R}+\alpha^{\overline{R}}\delta$; (2) remove $\gamma$ decay; (3) use differential return for value-functions.

\subsection{Policy Parameterization for Continuous Actions}

Instead of computing learned probabilities for each of the many actions, we instead learn statistics of the probability distribution.

Recall the probability density function is:
$$p(s)\doteq\frac{1}{\sigma\sqrt{2\pi}}exp(-\frac{(x-\mu)^2}{2\sigma^2})$$
To use PDF in policy parameterization
$$\pi(a|s,\bm{\theta}) \doteq\frac{1}{\sigma(s,\bm{\theta}) \sqrt{2\pi}}exp(-\frac{(a -\mu(s,\bm{\theta}))^2}{2\sigma(s,\bm{\theta})^2})$$
where we often declare that
$$\mu(s,\bm{\theta})\doteq \bm{\theta}_\mu^T \bm{x}_\mu (s)$$
$$\sigma(s,\bm{\theta}) \doteq exp(\bm{\theta}_\sigma^T \bm{x}_\sigma(s)) > 0$$

\subsection{Summary}

\begin{itemize}
\item In contrary to action-value methods, policy gradient methods perform action selections w/o consulting action-value estimates
\item Advantages of learn\&store policy parameters
\begin{itemize}
\item learn specific probabilities for taking the actions
\item learn appropriate levels of exploration
\item approach deterministic policies asymptotically
\item can handle continuous action spaces
\end{itemize}
\item policy gradient theorem provides a strong theoretical foundation
\item REINFORCE directly follows PG theorem
\item REINFORCE w/ baseline reduces variance w/o new bias
\item AC, critic introduces bias, but bootstrapping (TD) is superior than MC (substantially reduce variance)
\end{itemize}

\subsection{Learning Objectives (UA RL MOOC)}

Lesson 1: Learning Parameterized Policies 

1. Understand how to define policies as parameterized functions 

Parameterized policy: $\pi(a|s,\bm{\theta})$. Parameterized value function: $\hat{q}(s,a,\bm{w})$

Constraints on the policy parameterization
\begin{itemize}
\item $\pi(a|s,\bm{\theta}) \geq 0 \quad \forall a \in \A, s \in \S$
\item $\sum_{a\in\A} \pi(a|s,\bm{\theta}) = 1 \quad \forall s \in \S$
\end{itemize}

2. Define one class of parameterized policies based on the softmax function 

$$\pi(a|s,\bm{\theta})\doteq \frac{e^{h(s,a,\bm{\theta})}}{\sum_{b\in\A}e^{h(s,a,\bm{\theta})}}$$
$h$ function is action preference (it can be parameterized in any way), the exponential function guarantees the probability is positive for each action. The denominator normalizes the output of each action s.t. the sum over actions is one.
Note action preferences != action values; only the difference between preferences is important.

3. Understand the advantages of using parameterized policies over action-value based methods 

\begin{itemize}
\item Parameterized policies can autonomously decrease exploration over time. Specifically, the policy can start off stochastic to guarantee exploration; as learning progresses, the policy can naturally converge towards a deterministic greedy policy
\item They can avoid failures due to deterministic policies with limited function approximation
\item Sometimes the policy is less complicated than the value function
\end{itemize}

Lesson 2: Policy Gradient for Continuing Tasks 

4. Describe the objective for policy gradient algorithms 

Our objective:
$$r(\pi) = \sum_s \mu(s) \sum_a \pi(a|s,\bm{\theta}) \sum_{s',r} p(s',r|s,a)r $$
\begin{itemize}
\item $\sum_{s',r} p(s',r|s,a)r$ is $\E[R_t | S_t=s,A_t=a]$ expected reward if we start in state $s$ and take action $a$
\item $\sum_a \pi(a|s,\bm{\theta}) \sum_{s',r} p(s',r|s,a)r$ is $\E[R_t | S_t=s]$ expected reward of state $s$
\item $r(\pi)$ is $\E_\pi [ R_t ]$ overall average reward by considering the fraction of time we spend in state $s$ under policy $\pi$.
\end{itemize}
Optimizing the average reward objective (policy gradient method):
$$\nabla r(\pi) = \nabla \sum_s \mu(s) \sum_a \pi(a|s,\bm{\theta}) \sum_{s',r} p(s',r|s,a)r $$

The main challenge is modifying policy changes the distribution $\mu(s)$; in contrast, recall in $\overline{VE}$ objective, the distribution is fixed. We do gradient ascent for PG, while gradient descent in optimizing the mean squared value error.

5. Describe the results of the policy gradient theorem 

Note that chain rule of gradient requires to estimate $\nabla \mu(s)$, which is difficult to estimate since it depends on a long-term interaction between the policy and the environment. The PG theorem proves we don't need that, and following is our result:
$$\nabla r(\pi) = \sum_s \mu(s) \sum_a \nabla \pi(a|s,\bm{\theta}) q_\pi(s,a)$$
$\sum_a \nabla \pi(a|s,\bm{\theta}) q_\pi(s,a)$. This is a sum over the gradients of each action probability, weighted by the value of the associated action.
$r(\pi)$ takes the above expression and sum that over each state. This gives the direction to move the policy parameters to most rapidly increase the overall average reward.

6. Understand the importance of the policy gradient theorem 

Lesson 3: Actor-Critic for Continuing Tasks 

7. Derive a sample-based estimate for the gradient of the average reward objective 
$$\bm{\theta}_{t+1} \doteq \bm{\theta}_t + \alpha \frac{\nabla \pi(A_t | S_t, \bm{\theta_t})}{\pi(A_t | S_t, \bm{\theta_t})}q_\pi(S_t,A_t)$$

8. Describe the actor-critic algorithm for control with function approximation, for continuing tasks 

Full algorithm see chap 13.6

\begin{itemize}
\item actor: parameterized policy
\item critic: value functions, evaluating the actions selected by the actor
\end{itemize}
In the update rule, we don't have access to $q_\pi$, so we have to approximate it. For example, one-step bootstrap return TD method:
$$q_\pi(s,a) = R_{t+1} - \overline{R}+\hat{v}(S_{t+1}, \bm{w}) $$.
To further improve, we subtract a baseline to reduces the update variance (results in faster learning):
$$q_\pi(s,a) = R_{t+1} - \overline{R}+\hat{v}(S_{t+1}, \bm{w}) - \hat{v}(S_t, \bm{w}) $$.
Note this is equals the TD error $\delta$.

Lesson 4: Policy Parameterizations 

9. Derive the actor-critic update for a softmax policy with linear action preferences 
\begin{align*}
\bm{w} &= \bm{w} + \alpha^{\bm{w}} \delta \underbrace{\nabla\hat{v}(S,\bm{w})}_{\bm{x}(s)} \quad \text{just like semi-gradient TD}\\
\bm{\theta} &= \bm{\theta} + \alpha^{\bm{\theta}}\delta \underbrace{\nabla ln \pi(A|S,\bm{\theta})}_{ \bm{x}_h(s,a) - \sum_b \pi(b|s, \bm{\theta})x_h(s,b) }
\end{align*}

10. Implement this algorithm 

11. Design concrete function approximators for an average reward actor-critic algorithm 

12. Analyze the performance of an average reward agent 

13. Derive the actor-critic update for a gaussian policy 

Probability density means that for a given range, the probability of $x$ lying in that range will be the area under the probability density curve.

Gaussian Distribution
$$ p.d.f = p(x) \doteq \frac{1}{\sigma \sqrt{2\pi}}exp( -\frac{(x-\mu)^2}{2\sigma^2} )$$
$\mu$ = mean of distribution, $\sigma$ = standard deviation = $\sqrt{variance}$.
Gaussian Policy
$$ p.d.f = \pi(a|s,\bm{\theta}) \doteq \frac{1}{\sigma(s,\bm{\theta}) \sqrt{2\pi}}exp( -\frac{(a-\mu(s,\bm{\theta}))^2}{2\sigma(s,\bm{\theta})^2} )$$
$\mu(s,\bm{\theta})$ can be any parameterized function.
$\sigma$ controls the degree of exploration; usually initialized to be large s.t a wide range of actions are tried. As learning process, we expect the variance to shrink and the policy to concentrate around the best action in each state. The agent can reduce the amount of exploration through learning.

For $\theta$,
$$\nabla ln\pi(a|s, \bm{\theta}_\mu) = \frac{1}{\sigma(s,\bm{\theta})^2}(a-\mu(s,\bm{\theta}))\bm{x}(s)$$

For $\sigma$,
$$\nabla ln\pi(a|s, \bm{\theta}_\sigma) = (\frac{( a- \mu(s,\bm{\theta}) )^2}{\sigma(s,\bm{\theta})^2} - 1) \bm{x}(s)$$

Advantages of Continuous Actions
\begin{itemize}
\item it might not be straightforward to choose a proper discrete set of actions
\item continuous actions allow use to generalize over actions
\end{itemize}


14. Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions

For discrete actions, we use softmax policy parameterization; for continuous actions, we use Gaussian policy.



\end{document}