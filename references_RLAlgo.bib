@misc{haarnoja2018soft,
      title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
      author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
      year={2018},
      eprint={1801.01290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{mnih2013playing,
      title={Playing Atari with Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
      year={2013},
      eprint={1312.5602},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{haarnoja2019soft,
      title={Soft Actor-Critic Algorithms and Applications}, 
      author={Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and George Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
      year={2019},
      eprint={1812.05905},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@Misc{ytbcs287,
  Author = "Pieter Abbeel",
  Title  = "\emph{University of California, Berkeley CS287-FA19 Advanced Robotics}",
  Note   = "\url{https://www.youtube.com/watch?v=QASqaj_HUZw}
           [Accessed: 2021-12-07]",
  year = 2021,
}
@Misc{qlearningvspg,
  Author = "Neil Slater",
  Title  = "\emph{What is the relation between Q-learning and policy gradients methods?}",
  Note   = "\url{https://ai.stackexchange.com/a/6199}
           [Accessed: 2021-12-13]",
  year = 2021,
}
@article{schulman2017proximal,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  eprinttype = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{schulman2017trust,
      title={Trust Region Policy Optimization}, 
      author={John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
      year={2017},
      eprint={1502.05477},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@phdthesis{brianEntropy2010,
    title    = {Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy},
    school   = {Carnegie Mellon University},
    author   = {Brian D Ziebart},
    year     = {2010},
    type     = {{PhD} dissertation},
}
@misc{mnih2016asynchronous,
      title={Asynchronous Methods for Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
      year={2016},
      eprint={1602.01783},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{Williams1992SimpleSG,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Ronald J. Williams},
  journal={Machine Learning},
  year={1992},
  volume={8},
  pages={229-256}
}
@misc{haarnoja2017reinforcement,
      title={Reinforcement Learning with Deep Energy-Based Policies}, 
      author={Tuomas Haarnoja and Haoran Tang and Pieter Abbeel and Sergey Levine},
      year={2017},
      eprint={1702.08165},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{ahmed2019understanding,
      title={Understanding the impact of entropy on policy optimization}, 
      author={Zafarali Ahmed and Nicolas Le Roux and Mohammad Norouzi and Dale Schuurmans},
      year={2019},
      eprint={1811.11214},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{schulman2018equivalence,
      title={Equivalence Between Policy Gradients and Soft Q-Learning}, 
      author={John Schulman and Xi Chen and Pieter Abbeel},
      year={2018},
      eprint={1704.06440},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{odonoghue2017combining,
      title={Combining policy gradient and Q-learning}, 
      author={Brendan O'Donoghue and Remi Munos and Koray Kavukcuoglu and Volodymyr Mnih},
      year={2017},
      eprint={1611.01626},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{nachum2017bridging,
      title={Bridging the Gap Between Value and Policy Based Reinforcement Learning}, 
      author={Ofir Nachum and Mohammad Norouzi and Kelvin Xu and Dale Schuurmans},
      year={2017},
      eprint={1702.08892},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@Misc{ppo2stable,
  Author = "Antonin Raffin",
  Title  = "\emph{PPO2 Stable Baselines 2.10.2 documentation}",
  Note   = "\url{https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html}
           [Accessed: 2021-11-26]",
  year = 2021,
}
@Misc{ppo2vsppo1,
  Author = "pzhokhov",
  Title  = "\emph{[PPO2] What is the difference between PPO1 and PPO2?}",
  Note   = "\url{https://github.com/openai/baselines/issues/485}
           [Accessed: 2021-11-26]",
  year = 2021,
}
@Misc{ppolclip,
  Author = "matwilso",
  Title  = "\emph{What is the way to understand Proximal Policy Optimization Algorithm in RL?}",
  Note   = "\url{https://stackoverflow.com/a/50663200}
           [Accessed: 2021-11-26]",
  year = 2021,
}
@Misc{ppocs294,
  Author = "John Schulman",
  Title  = "\emph{University of California, Berkeley CS294-112 11/20/17}",
  Note   = "\url{https://www.youtube.com/watch?v=gqX8J38tESw}
           [Accessed: 2021-11-28]",
  year = 2021,
}
@misc{hsu2020revisiting,
      title={Revisiting Design Choices in Proximal Policy Optimization}, 
      author={Chloe Ching-Yun Hsu and Celestine Mendler-Dünner and Moritz Hardt},
      year={2020},
      eprint={2009.10897},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{weng2018PG,
  title   = "Policy Gradient Algorithms",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io/lil-log",
  year    = "2018",
  url     = "https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html"
}
@Inbook{Zhang2020,
author="Zhang, Hongming
and Yu, Tianyang",
editor="Dong, Hao
and Ding, Zihan
and Zhang, Shanghang",
title="Taxonomy of Reinforcement Learning Algorithms",
bookTitle="Deep Reinforcement Learning: Fundamentals, Research and Applications",
year="2020",
publisher="Springer Singapore",
address="Singapore",
pages="125--133",
abstract="In this chapter, we introduce and summarize the taxonomy and categories for reinforcement learning (RL) algorithms. Figure 3.1 presents an overview of the typical and popular algorithms in a structural way. We classify reinforcement learning algorithms from different perspectives, including model-based and model-free methods, value-based and policy-based methods (or combination of the two), Monte Carlo methods and temporal-difference methods, on-policy and off-policy methods. Most reinforcement learning algorithms can be classified under different categories according to the above criteria, hope this helps to provide the readers some overviews of the full picture before introducing the algorithms in detail in later chapters.Fig. 3.1Map of reinforcement learning algorithms. Boxes with thick lines denote different categories, others denote specific algorithms",
isbn="978-981-15-4095-0",
doi="10.1007/978-981-15-4095-0_3",
url="https://doi.org/10.1007/978-981-15-4095-0_3"
}
@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}



