\documentclass[sutton_barto_notes.tex]{subfiles}
\begin{document}


\newpage
\section{Planning and Learning with Tabular Methods}

\begin{itemize}
\item model-based: rely on \textit{planning}; require a model of the environment (anything that an agent can use to predict how the environment will respond to its actions), e.g. dynamic programming and heuristic search
\item model-free: rely on \textit{learning}; use without a model, such as Monte Carlo and temporal-difference method
\end{itemize}


\subsection{Models and Planning}

\begin{itemize}
\item distribution model: models that produce a description of all possibilities and their probabilities (e.g. MDP dynamics $p(s',r|s,a)$
\item sample models: models that produce just one of the possibilities, sampled according to the probabilities (e.g. HTHTHH flipping coin sequence)
\end{itemize}

Distribution models are stronger than sample models in that they can always be used to produce samples; however, sample models are easier to implement.

\begin{definition}
\textbf{planning}, refer to any computational process that takes a \textbf{model} as input and produces or \textbf{improves a policy} for interacting with the modeled environment.
$$\text{model} \xrightarrow[]{\text{planning}} \text{policy} $$
\end{definition}

\begin{itemize}
\item state-space planning: a search through the state space for an $\pi_*$, or an optimal path to a goal
\item plan-space planning: a search through the space of plans
\begin{itemize}
	\item includes evolutionary methods, and partial-order planning (ordering of steps is not completely determined at all stages of planning)
	\item hard to apply to the stochastic sequential decision problems
	\item not focused in this book
\end{itemize}
\end{itemize}

State-space planning methods' structure: (1) all state-space planning methods involve computing value functions to improve the policy, (2) they compute value functions by updates or backup operations applied to simulated experience.
$$ \text{model} \rightarrow \text{simul. exp.} \rightarrow \xrightarrow[]{\text{backup}} \rightarrow \text{values} \rightarrow \text{policy} $$
State-space planning methods fits in the above structure, only differed by (1) update rule, (2) order of update, (3) how long the backed-up info is retained.

The heart of both planning and learning methods is the estimation of value functions by backing-up update operations.
\begin{itemize}
\item planning uses simulated experience generated by a model (e.g. DP)
\item learning uses real experience generated by the environment (e.g. TD)
\end{itemize}


\begin{tcolorbox}[width=1.1\textwidth,title={Random-sample one-step tabular Q-\textbf{planning}}]
Loop forever:

$\quad$1. Select a state, $S \in \S$, and an action, $A \in \A(\S)$, at random

$\quad$2. Send $S$, $A$ to a sample model, and obtain a sample next $R$, and $S'$

$\quad$Apply one-step tabular Q-\textbf{learning} to $S,A,R,S'$:

$\quad\quad Q(S,A)\leftarrow Q(S,A) + \alpha [R + \gamma max_a Q(S',a) - Q(S,A)]$
\end{tcolorbox}


\subsection{Dyna: Integrated Planning, Acting, and Learning}

\textbf{Problem}: Both decision making and model learning are computation-intensive

\textbf{Solution}: To balance these two, we use a architecture to integrate the major functions in an online planning agent, called Dyna-Q.

\begin{figure}[!h]
    \centering
    \subfigure[]{\includegraphics[width=0.45\textwidth]{Dyna_struct_1.png}}
    \subfigure[]{\includegraphics[width=0.45\textwidth]{Dyna_struct_2.png}}
    \caption{ (a) Dyna, balancing decision making and model learning (b) General Dyna Architecture }
    \label{fig:dyna}
\end{figure}

\begin{itemize}
\item real experience: (1) \textit{model-learning} (or indirect RL) to improve model (more accurately match the real environment); (2) \textit{direct RL} to improve value function and policy
\begin{itemize}
	\item indirect methods: maximize the use of limited experience (better policy with fewer environmental interactions)
	\item direct methods are more simpler and are not affected by biases incurred by the model
\end{itemize}
\item all planning, acting, model-learning, and direct RL occurring continually
\begin{itemize}
	\item planning here uses random-sample one-step tabular Q-\textbf{planning}
	\item direct RL here uses one-step tabular Q-\textbf{learning}
\end{itemize}
\item \textit{search control} to refer to the process that selects the starting states and actions for the simulated experiences generated by the model
\end{itemize}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.6\linewidth]{dyna_3.png}
  \label{fig:dyna}
\end{figure}

The Dyna Q uses both real world experience (which is expensive) and simulated (hallucinated) experience (which is cheap; more iterations are completed with simulated experience), thus accelerating training. The real experience is for learning, and simulated experience is for planning.

\begin{tcolorbox}[width=1.1\textwidth,title={Tabular Dyna-Q}]
Init $Q(s,a)$ and $Model(s,a)$ for all $s \in \S$ and $a \in \A(s)$

Loop forever:

$\quad$(a) $S \leftarrow$ current (nonterminal) state

$\quad$(b) $A \leftarrow \epsilon$-greedy$(S,Q)$

$\quad$(c) Take action $A$; observe resultant reward, $R$, and state, $S'$

$\quad$(d) $Q(S,A)\leftarrow Q(S,A)+\alpha [R + \gamma max_a Q(S',a) - Q(S,A)]$

$\quad$(e) $Model(S,A) \leftarrow R,S'$ (assuming deterministic environment)

$\quad$(f) Loop repeat $n$ times:

$\quad\quad$ $S \leftarrow$ random previously observed state

$\quad\quad$ $A \leftarrow$ random action previously taken in $S$

$\quad\quad$ $R, S' \leftarrow Model(S,A)$

$\quad\quad$ $Q(S,A)\leftarrow Q(S,A)+\alpha [R+\gamma max_a Q(S',a) - Q(S,A)]$
\end{tcolorbox}

\subsection{When the Model Is Wrong}

The model learning in the Dyna-Q may be incorrect when encountering stochastic environment. The model may not be exploratory enough to find the new (optimal) path when environment changes, sticking to the old path. We slightly modified the reward transition in Dyna-Q+ with $r+\kappa\sqrt{\tau}$ for small $\kappa$, where $\tau$ is the time steps that the $(s,a)$ has not been tried (similar to how UCB incorporate time steps inside).

\subsection{Learning Objectives (UA RL MOOC)}


Lesson 1: What is a model? 

1. Describe what a model is and how they can be used

\begin{itemize}
\item Model are used to store knowledge about the transition and reward dynamics
\item Given $S,A$ into model, model outputs $R, S'$
\item A model allows for planning
\item Planning refers to the process of using a model to improve a policy (use model to simulate experience, update value functions with the simulated experience, then improve policy with the updated value functions)
\end{itemize}

2. Classify models as distribution models or sample models 

\begin{itemize}
\item Sample models procedurally generate samples, without explicitly storing the probability of each outcome (e.g. flip coin twice: HT)
\item Distribution models contain a list of all outcomes and their probabilities (e.g. flip coin twice HT(1/4),HH(1/4),TT(1/4),TH(1/4))
\end{itemize}

3. Identify when to use a distribution model or sample model 

Sample model can be computationally inexpensive. Distribution model contains more info, but it's hard to specify and can become large.

4. Describe the advantages and disadvantages of sample models and distribution models 

Sample models require less memory

Distribution models can be used to compute the exact expected outcome (note sample models have to averaging many samples to get an approximate). Can be used to access risk.

5. Explain why sample models can be represented more compactly than distribution models

Consider rolling dice sample. The more dice there are, the larger the state space.

Lesson 2: Planning 

6. Explain how planning is used to improve policies 

Planning uses simulated experience from model to improve policies.

7. Describe random-sample one-step tabular Q-planning 

(1) sample from model; (2) Q-learning update; (3) Greedy policy improvement

Lesson 3: Dyna as a formalism for planning 

8. Recognize that direct RL updates use experience from the environment to improve a policy or value function 

Direct RL, like Q-learning, directly learn from real world experience (environment)

9. Recognize that planning updates use experience from a model to improve a policy or value function 

Indirect RL, like Q-planning, learn from simulated experience (generated by model) to improve value function and policy.

10. Describe how both direct RL and planning updates can be combined through the Dyna architecture

see \ref{fig:dyna} we do direct RL with real experience to improve policy or value functions; and we learn a model use real experience, then we sample (state, action) from the model which we use to get simulating experience, and we do planning update based on simulated experience to update policy or value functions

11. Describe the Tabular Dyna-Q algorithm 

see Chap 8.2

12. Identify the direct-RL and planning updates in Tabular 

direct RL update is the step (d), planning update is step (f)

13. Identify the model learning and search control components of Tabular Dyna-Q 

10-13 see chap 8.2

14. Describe how learning from both direct and simulated experience impacts performance 

It accelerate learning and it also very sample efficient.

15. Describe how simulated experience can be useful when the model is accurate 

It can provide more samples for training, thus it's improve sample efficiency.

Lesson 4: Dealing with inaccurate models 

16. Identify ways in which models can be inaccurate 

Models are inaccurate when transitions they store are different from transitions that happen in the environment (like haven't explore enough).

17. Explain the effects of planning with an inaccurate model 

At first, the model is incomplete. As the agent interacts with the environment, the model stores more and more transitions. Then, the agent can perform updates by simulating transitions it's seen before. That means that as long as the agent has seen some transitions, it can plan with the model.

If the agent plan to with inaccurate model, then the value function or policy that the agent updates might change in the wrong direction. Planning with inaccurate model can make the policy worse w.r.t the environment.

18. Describe how Dyna can plan successfully with a partially inaccurate model 

In the step (f), the model only knows the next state and reward from $(s,a)$ it has already visited; therefore, Dyna-Q can only do planning updates from previously visited $(s,a)$ pairs. Dyna-Q only plans the transition it has already seen. So in the first few timesteps of learning, Dyna-Q might do quite a few planning updates with the same transition. However, as Dyna-Q visits more $(s,a)$ in the environment, its planning updates become more evenly distributed throughout the state action space.

19. Explain how model inaccuracies produce another exploration-exploitation trade-off 

Explore to make sure it's model is accurate, exploit the model to compute the optimal policy, assuming that the model is correct.

20. Describe how Dyna-Q+ proposes a way to address this trade-off

We add a bonus rewards for exploration, new reward = $r + \kappa\sqrt{\tau}$, where $\kappa$ is small constant and $\tau$ is timesteps since transition was last tried.



\end{document}